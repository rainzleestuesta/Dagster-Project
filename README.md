# NBA Stats Dagster Pipeline

This repository implements an end-to-end data pipeline for NBA analytics using Dagster. It scrapes, cleans, merges, validates, processes, and visualizes NBA player and team statistics, and schedules automated runs.


## 🚀 Project Structure
```
dagster_project/
├── assets/
│   ├── merge.py                   # Loads merged CSV into DataFrame
│   ├── validate.py                # Data quality checks
│   ├── process.py                 # Computes all-around z-scores
│   ├── insights.py                # Visualization assets
├── io_managers.py                 # CSV I/O manager (load & save)
├── schedules.py                   # Dagster schedule & job definition
├── repository.py                  # Defines assets, job, schedule
├── workspace.yaml                 # Points to repository.py
├── preprocess/                    # CSV & output folder
│   ├── merged_players_team_stats.csv
│   ├── processed_stats.csv        # Generated by pipeline
│   ├── validated_stats.csv        # Validated data
|   ├── Scraped csv files          # Scraped data
|   ├── Python Scrapers            # Python codes used in scraping data
│   └── *.png                      # Insight charts
└── README.md
```

---

## 🛠️ Setup & Installation

1. **Clone the repo**
   ```bash
   git clone https://github.com/rainzleestuesta/Dagster-Project
   cd dagster_project
   ```
2. **Create & activate a virtual environment**
   ```bash
   python -m venv .venv
   source .venv/bin/activate      # macOS/Linux
   .venv\Scripts\activate       # Windows
   ```
3. **Install dependencies**
   ```bash
   pip install dagster dagster-webserver pandas matplotlib
   ```

---

## ▶️ Running Locally

1. **Start Dagster**
   ```bash
   dagster dev
   ```
2. **Open the UI**
   Navigate to http://localhost:3000
3. **Materialize Pipeline**
   - Under **Jobs**, select **stats_pipeline** and click **Launch Run**
   - Or under **Assets**, click **Materialize All**

Pipeline phases:
- **merged_stats**: load `merged_players_team_stats.csv`
- **validated_stats**: data quality checks
- **player_performance_metrics**: compute all-around z-scores
- **insights**: generate charts in `preprocess/*.png`

---

## 📅 Schedule

The pipeline runs daily at 09:00 via the `daily_csv_schedule` (cron `0 9 * * *`).

---

## ✅ Data Quality Checks

- No nulls in key columns: `player`, `team_code`, `pts_per_game`, `win_percentage`
- `pts_per_game` between 0 and 40
- `win_percentage` between 0.0 and 1.0

---

## 📈 Insights & Outputs

- **Top 10 All-Around Players Based on z-scores**: `preprocess/top_all_around.png`
- **Top Scorers**: `preprocess/top_scorers.png`
- **Top Assist Leaders**: `preprocess/top_assists.png`
- **Team PPG Ranking**: `preprocess/team_ppg_ranking.png`
- **Best offensive teams**: `preprocess/best_offense.png`
- **Team Pace Ranking**: `preprocess/team_pace_ranking.png`
- **Net Rating vs Win% scatter plotr**: `preprocess/netrating_vs_win.png`

---

## 🔄 Extending the Pipeline

- Add new assets in `assets/`
- Register them in `repository.py`
- Update `io_managers.py` for custom I/O
- Define new schedules in `schedules.py`

---

Happy analytics! 🎉

